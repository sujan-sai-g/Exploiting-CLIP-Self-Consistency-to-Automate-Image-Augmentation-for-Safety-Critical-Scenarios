# -*- coding: utf-8 -*-
"""

"""
from __future__ import annotations

import argparse
import os
import sys
import numpy as np
from PIL import Image
import torch
import tqdm
import time
import random

# add path for controlnet extension
path_webui = os.path.dirname(os.path.realpath(__file__))
controlnet_path_script = path_webui + "/extensions/sd-webui-controlnet"
#print(controlnet_path_script)
sys.path.append(controlnet_path_script)

# initialization from webui
from modules import timer
from modules import initialize

startup_timer = timer.startup_timer
startup_timer.record("launcher")
initialize.imports()

# load other webui libraries
from modules.cmd_args import parser
from modules import scripts
from modules import codeformer_model

#codeformer_model.setup_model(dirname="CodeFormer")
from modules.processing import StableDiffusionProcessingImg2Img, process_images
from modules.sd_models import CheckpointInfo, load_model, read_state_dict
from modules import shared
from modules.lang_sam.lang_sam import LangSAM

# load webui extension libraries
from scripts.controlnet import Script as ControlScript
from internal_controlnet.external_code import ControlNetUnit
from modules.processing_scripts import refiner, seed
from scripts import data_generation_utils
np.random.seed(42)

# Set seed for PyTorch
torch.manual_seed(42)

# Set seed for Python's built-in random module
random.seed(42)


if __name__ == "__main__":
    starting_time = time.time()
    torch.cuda.empty_cache()
    scripts.load_scripts()
    args = parser.parse_args()

    # critical errors:
    if args.filepath_model is None:
        raise ValueError
    if args.input_image_path is None:
        raise ValueError    
    
    # batch_size
    if args.batch_size is None:
        args.batch_size = 1
    
    # args.prompt = args.prompt * args.batch_size
    
    image_paths = data_generation_utils.get_images(args)

    # stable diffusion model
    checkpoint_info = CheckpointInfo(filename=args.filepath_model)
    checkpoint_info.register()
    state_dict = read_state_dict(checkpoint_info.filename)
    shared.sd_model = load_model(checkpoint_info=checkpoint_info)
    shared.sd_model.lowvram = False
    #shared.cmd_opts.force_enable_xformers = True
    
    # controlnet model
    args.controlnet_name = args.controlnet_name.replace(".pth", "")
    controlnetscript_object = ControlScript()
    controlnetscript_object.args_from = 10
    controlnetscript_object.args_to = 13
    my_controlnet = ControlNetUnit(model=args.controlnet_name,
                                   low_vram = False,
                                   resize_mode="Crop and Resize",
                                   #processor_res=512,
                                   control_mode="ControlNet is more important")
    
    # refiner script
    scriptrefiner_object = refiner.ScriptRefiner()
    scriptrefiner_object.argsfrom = 1
    scriptrefiner_object.argsto = 4
    
    # seed script
    scriptseed_object = seed.ScriptSeed()
    scriptseed_object.argsfrom = 4
    scriptseed_object.argsfrom = 10
    
    if args.mask_height is None:
        args.mask_height = 300
    if args.mask_width is None:
        args.mask_width = 150

    # loop over images in green
    for idx, file_path in tqdm.tqdm(enumerate(image_paths), colour="green"):
        torch.cuda.empty_cache()
        
        # image
        print(file_path)
        init_images = []
        with Image.open(file_path) as image:
            
            image = data_generation_utils.scale_image(image, new_width=768, new_height=512)
            image_array = np.asarray(image)
            image_rgb = Image.fromarray(image_array, mode="RGB")
            
            init_images.append(image_rgb)            
        
        # positive prompts
        random_prompt = data_generation_utils.get_prompt()
        print(random_prompt)
        prompt_list = list()
        prompt_list.append(random_prompt)    
        prompt_list = prompt_list * args.batch_size
        # negative prompt
        if args.negative_prompt is None:
            args.negative_prompt = "nudity, naked, sex, vulgar, obscene, sexual"
        
        # control_image
        pose_array = data_generation_utils.get_controlnet_pose_image(args, prompt_list[0])
        
        # mask
        mask, pose_image, skip_flag = data_generation_utils.define_mask(args, init_images, pose_array, file_path)        
        if not skip_flag:
            # seed
            if args.seed is None:
                args.seed = -1
            
            # sampler
            if args.sampler_name is None:
                args.sampler_name = "Euler a"
            
            prompt_styles = []
            n_iter = 1
            steps = 50
            cfg_scale = 5
            width = 512
            height = 512
            mask_blur = 4
            inpainting_fill = 1
            resize_mode = 0
            denoising_strength = 0.75
            image_cfg_scale = 0.7
            inpaint_full_res = 1
            inpaint_full_res_padding = 32
            inpainting_mask_invert = 0
            override_settings = {}
            
            p = StableDiffusionProcessingImg2Img(
                sd_model=shared.sd_model,
                outpath_samples= args.output_image_path,
                outpath_grids=shared.opts.outdir_grids or shared.opts.outdir_img2img_grids,
                prompt=prompt_list,
                negative_prompt=args.negative_prompt,
                styles=prompt_styles,
                sampler_name=args.sampler_name,
                batch_size=1, # for some reason using batch size crashed. so we just run with same prompt and image multiple times 
                n_iter=n_iter,
                steps=steps,
                cfg_scale=cfg_scale,
                width=width,
                height=height,
                init_images=init_images,
                mask=mask,
                mask_blur=mask_blur,
                inpainting_fill=inpainting_fill,
                resize_mode=resize_mode,
                denoising_strength=denoising_strength,
                image_cfg_scale=image_cfg_scale,
                inpaint_full_res_padding=inpaint_full_res_padding,
                inpainting_mask_invert=inpainting_mask_invert,
                override_settings=override_settings,
                inpaint_full_res=inpaint_full_res,
            )
            # set arguments the way stable diffusion webui handles them from GUI
            # your best option to find these is to run the GUI and print them out in img2img.py img2img()
            p_args = (0, False, '', 0.8, 42, False, -1, 0, 0, 0, my_controlnet, controlnetscript_object.get_default_ui_unit(), controlnetscript_object.get_default_ui_unit(), '* `CFG Scale` should be 2 or lower.', True, True, '', '', True, 50, True, 1, 0, False, 4, 0.5, 'Linear', 'None', '<p style="margin-bottom:0.75em">Recommended settings: Sampling Steps: 80-100, Sampler: Euler a, Denoising strength: 0.8</p>', 128, 8, ['left', 'right', 'up', 'down'], 1, 0.05, 128, 4, 0, ['left', 'right', 'up', 'down'], False, False, 'positive', 'comma', 0, False, False, '', '<p style="margin-bottom:0.75em">Will upscale the image by the selected scale factor; use width and height sliders to set tile size</p>', 64, 0, 2, 1, '', [], 0, '', [], 0, '', [], True, False, False, False, 0, False, None, None, False, None, None, False, None, None, False, 50)
            p.scripts = scripts.scripts_img2img
            p.script_args = p_args
            
            p.controlnet_input_image = pose_array
            p.seed = args.seed
            # face restoration
            p.restore_faces = False
            #face_restorers = []
            #for fr in shared.face_restorers:
            #    if isinstance(fr, str):
            #        continue
            #    face_restorers.append(fr)
            #shared.face_restorers = face_restorers
            p.is_using_inpainting_conditioning=False
            p.paste_to = None
            p.seed_enable_extras = True
            p.tiling = None
            p.restore_faces = None
            p.do_not_save_samples = False
            p.do_not_save_grid = False
            p.overlay_images = None
            p.eta = None
            p.do_not_reload_embeddings = False
            p.ddim_discretize = None
            p.s_min_uncond = 0.0
            p.s_churn = 0.0
            p.s_tmax = "inf"
            p.s_tmin = 0.0
            p.s_noise = 1.0
            p.override_settings_restore_afterwards = True
            p.sampler_index = None
            p.refiner_checkpoint = None
            p.refiner_switch_at = None
            p.disable_extra_networks = False
            p.comments = {}
            p.mask_blur_x = 4
            p.mask_blur_y = 4
            p.initial_noise_multiplier = 1.0
            p.latent_mask = None
            p.sampler_noise_scheduler_override = None
            p.refiner_checkpoint_info = None
            p.cached_uc = [None, None]
            p.cached_c = [None, None]
            p.user = None
            
            #p.scripts.alwayson_scripts = [scriptrefiner_object, scriptseed_object, controlnetscript_object] # what does htis do ? 
            
            default_seed = None
            batch_index = 0
            for prompt in tqdm.tqdm(prompt_list, colour="blue"):
                p.prompt = prompt
                filename = file_path.split("/")[-1].split(".")[0]
                age = prompt_list[0].split(" ")[1]
                gender = prompt_list[0].split(" ")[2]
                skincolor = prompt_list[0].split(" ")[3]
                shirtcolor = prompt_list[0].split(" ")[6]
                action = prompt_list[0].split(" ")[9]
                final_filename = filename + "_" + f"{age}_{gender}_{skincolor}_{shirtcolor}_{action}"
                p.outpath_samples= args.output_image_path + "/" + final_filename
                """ if default_seed is None:
                    if p.seed != -1:
                        default_seed = p.seed
                else:
                    p.seed = default_seed + batch_index """
                p.seed = 123 + batch_index
                batch_index += 1
                batch_index = batch_index % args.batch_size
                print(f"batch_index is {batch_index}")
                print(f"seed is {123 + batch_index}")
                torch.cuda.empty_cache()
                # start the process
                #p_args = (0, False, '', 0.8, 42, False, -1, 0, 0, 0, my_controlnet, controlnetscript_object.get_default_ui_unit(), controlnetscript_object.get_default_ui_unit(), '* `CFG Scale` should be 2 or lower.', True, True, '', '', True, 50, True, 1, 0, False, 4, 0.5, 'Linear', 'None', '<p style="margin-bottom:0.75em">Recommended settings: Sampling Steps: 80-100, Sampler: Euler a, Denoising strength: 0.8</p>', 128, 8, ['left', 'right', 'up', 'down'], 1, 0.05, 128, 4, 0, ['left', 'right', 'up', 'down'], False, False, 'positive', 'comma', 0, False, False, '', '<p style="margin-bottom:0.75em">Will upscale the image by the selected scale factor; use width and height sliders to set tile size</p>', 64, 0, 2, 1, '', [], 0, '', [], 0, '', [], True, False, False, False, 0, False, None, None, False, None, None, False, None, None, False, 50)
                #result = p.scripts.run(p, *p_args)

                p.override_settings['samples_filename_pattern'] = f'{final_filename}_{batch_index}'    

                result = p.scripts.run(p, *p.script_args)
                
                if result is None:
                    result = process_images(p)
        else:
            pass

    time = time.time() - starting_time
    print(f"time taken {time}")