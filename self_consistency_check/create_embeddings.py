import glob

import clip
import pandas as pd
import torch
import tqdm
from PIL import Image


def embeddings_gen(args):

    filenames = glob.glob(args.image_crop_path + "*.png")
    model, preprocess = clip.load("ViT-L/14", device=args.device)

    filenames.sort()

    def run_inference(image):
        with torch.no_grad():            
            image_features = model.encode_image(image)
        return image_features

    gen_instance_id_list = []
    filename_list = []
    image_features_list = []
    for _, item in tqdm.tqdm(enumerate(filenames)):
        image = preprocess(Image.open(item)).unsqueeze(0).to(args.device)
        image_features = run_inference(image)
        image_features_list.append(image_features.cpu().numpy())
        gen_instance_id_list.append(item.split("/")[-1].split("_")[-1].split('.')[0])
        filename_list.append('_'.join(item.split("/")[-1].split("_")[0:4]).split(".")[0].split("-")[-1])

    df = pd.DataFrame({
        'Filename': filename_list,
        'File_gen_instance_id': gen_instance_id_list,
        'image_features_list': image_features_list
    })
    df.to_pickle(args.image_embeddings_path + "generated_cityscapes_embedding.pkl")
