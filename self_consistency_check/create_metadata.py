import argparse

import clip
import numpy as np
import pandas as pd
import torch
import tqdm

from prompts import pedestrian_prompts
from PIL import Image


class CLIPAnnotator:
    def __init__(self, args):
        self.device = args.device
        self.image_embeddings_df = pd.read_pickle(args.image_embeddings_path + "generated_cityscapes_embedding.pkl")
        self.load_clip_model()
        self.convert_image_embeddings_to_features()

    def load_clip_model(self):
        self.model, self.preprocess = clip.load("ViT-L/14", device=self.device)

    def convert_image_embeddings_to_features(self):
        image_tensor = torch.tensor(self.image_embeddings_df['image_features_list'])
        self.image_features = torch.squeeze(image_tensor).to(self.device)

    def convert_text_embeddings_to_features(self, prompts):
        text = clip.tokenize(prompts).to(self.device)
        self.text_features = self.model.encode_text(text)

    def calculate_similarity(self):
        with torch.no_grad():
            self.image_features /= self.image_features.norm(dim=-1, keepdim=True)
            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)
            similarity = (100.0 * self.image_features @ self.text_features.T).softmax(dim=-1)
            self.logits = self.image_features @ self.text_features.T
        return similarity

    def get_scores(self, prompts, num_attributes):
        self.convert_text_embeddings_to_features(prompts)
        similarity = self.calculate_similarity()

        num_prompts_per_attribute = int((len(prompts) / num_attributes))
        reshaped_similarity = similarity.reshape(-1, num_attributes, num_prompts_per_attribute)

        mean_similarity = reshaped_similarity.mean(axis=-1)
        preds = np.argmax(mean_similarity.cpu().numpy(), axis=1)

        return preds


class MetadataGen:
    """
    Generates single csv file with object + metadata given object ontology and image embeddings
    """

    def __init__(self, args):
        self.csv_outputs = args.csv_outputs
        self.ontology_path = args.ontology_path
        self.device = args.device
        self.image_embeddings_df = pd.read_pickle(args.image_embeddings_path + "generated_cityscapes_embedding.pkl")
        self.clip_anno = CLIPAnnotator(args)

        self.extract_sem_dims_from_ontology()
        self.initialize_df()

    def read_ontology_of_object(self):
        self.ontology = pd.read_csv(self.ontology_path + "pedestrian_ontology.csv")        
        valid_items_count = self.ontology.count(axis=1)
        self.ontology['num_attributes'] = valid_items_count - 1

    def extract_sem_dims_from_ontology(self):
        self.read_ontology_of_object()

        self.sem_dims = self.ontology['sem-dim'].to_list()

    def prompt_generation_for_sem_dim(self, sem_dim):
        prompts = pedestrian_prompts.prompts[sem_dim]       
        return prompts

    def initialize_df(self):
        col_names = {col: [] for col in self.sem_dims}
        self.df = pd.DataFrame(col_names)
        self.df['Filename'] = self.image_embeddings_df['Filename']
        self.df['File_gen_instance_id'] = self.image_embeddings_df['File_gen_instance_id']

        last_two_columns = self.df.columns[-2:]
        # Reorder the DataFrame columns
        self.df = self.df[
            last_two_columns.tolist() + [col for col in self.df.columns if col not in last_two_columns]]

    def run(self):
        for idx, sem_dim in tqdm.tqdm(enumerate(self.sem_dims)):
            prompts_per_dim = self.prompt_generation_for_sem_dim(sem_dim)
            scores = self.clip_anno.get_scores(prompts_per_dim, self.ontology['num_attributes'][idx])

            self.df[sem_dim] = scores
        self.save_generated_metadata_file()

    def save_generated_metadata_file(self):
        self.df.to_csv(self.csv_outputs + "generated_cityscapes_metadata.csv")